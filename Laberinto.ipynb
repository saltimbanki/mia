{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGxHDCuVuUrX"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------------------------------------\n",
        "# Agregamos las liberías\n",
        "#----------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Definimos las acciones\n",
        "#----------------------------------------------------------------------------\n",
        "acciones = [ ( 0 , 1 ), ( 0, -1 ), ( 1, 0 ), ( -1, 0 ) ]  # Derecha, Izquierda, Abajo, Arriba\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Función para elegir una acción epsilon-greedy\n",
        "#----------------------------------------------------------------------------\n",
        "def elegir_accion( Q, epsilon, posicion ):\n",
        "    if np.random.uniform( 0, 1) < epsilon:\n",
        "        return np.random.choice( len( acciones ) )\n",
        "    else:\n",
        "        return np.argmax( Q[ posicion[ 0 ], posicion[ 1 ] ] )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Función para actualizar la tabla Q usando Q-learning\n",
        "#----------------------------------------------------------------------------\n",
        "def actualizar_Q( Q, posicion, accion, recompensa, proxima_posicion, alpha, gamma ):\n",
        "    max_next_Q = np.max( Q[ proxima_posicion[ 0 ], proxima_posicion[ 1 ] ] )\n",
        "    Q[ posicion[ 0 ], posicion[ 1 ], accion ] = (1 - alpha) * Q[ posicion[ 0 ], posicion[ 1 ], accion ] + alpha * (\n",
        "                recompensa + gamma * max_next_Q )\n",
        "    return Q\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Función para encontrar el camino más corto usando la política aprendida (greedy)\n",
        "#----------------------------------------------------------------------------\n",
        "def encontrar_camino_mas_corto( Q, laberinto ):\n",
        "    posicion = (1, 1)\n",
        "    camino_mas_corto = [ posicion ]\n",
        "    while laberinto[ posicion[ 0 ], posicion[ 1 ]] != \"M\":\n",
        "        accion = np.argmax( Q[ posicion[ 0 ], posicion[ 1 ] ] )\n",
        "        proxima_posicion = ( posicion[ 0 ] + acciones[ accion ][ 0 ], posicion[ 1 ] + acciones[ accion ][ 1 ] )\n",
        "        camino_mas_corto.append( proxima_posicion )\n",
        "        posicion = proxima_posicion\n",
        "    return camino_mas_corto\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Función para comprobar la conectividad entre dos puntos en el laberinto\n",
        "#----------------------------------------------------------------------------\n",
        "def es_conectado( laberinto, inicio, fin ):\n",
        "    visitado = set()\n",
        "    fila_inicial, columna_inicial = inicio\n",
        "    fila_final, columna_final = fin\n",
        "    queue = deque( [ ( fila_inicial, columna_inicial ) ] )\n",
        "    while queue:\n",
        "        fila, columna = queue.popleft()\n",
        "        if ( fila, columna)  == ( fila_final, columna_final ):\n",
        "            return True\n",
        "        if ( fila, columna ) in visitado or laberinto[ fila, columna ] == '#':\n",
        "            continue\n",
        "        visitado.add( ( fila, columna ) )\n",
        "        for accion in acciones:\n",
        "            nueva_fila, nueva_columna = fila + accion[ 0 ], columna + accion[ 1 ]\n",
        "            if 0 <= nueva_fila < laberinto.shape[ 0 ] and 0 <= nueva_columna < laberinto.shape[ 1 ]:\n",
        "                queue.append( ( nueva_fila, nueva_columna ) )\n",
        "    return False\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Función para generar un nuevo laberinto\n",
        "#----------------------------------------------------------------------------\n",
        "def generar_laberinto( tamaño, obstaculos ):\n",
        "    laberinto = np.full( ( tamaño, tamaño ), ' ' )\n",
        "    laberinto[ 0, : ] = laberinto[ -1, : ] = laberinto[ :, 0 ] = laberinto[ :, -1 ] = '#'  # Paredes exteriores\n",
        "    laberinto[ 1, 1 ] = 'S'  # Posición inicial\n",
        "    laberinto[ -2, -2 ] = 'M'  # Posición final\n",
        "    for _ in range( obstaculos ):\n",
        "        fila, columna = np.random.randint( 1, tamaño - 1 ), np.random.randint( 1, tamaño - 1 )\n",
        "        if laberinto[ fila ][ columna ] == ' ':\n",
        "            laberinto[ fila ][ columna ] = \"#\"\n",
        "    return laberinto\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Definición del laberinto\n",
        "#----------------------------------------------------------------------------\n",
        "tamaño = 20\n",
        "obstaculos = 50\n",
        "laberinto = generar_laberinto( tamaño, obstaculos )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Comprobamos la conectividad entre S y M\n",
        "#----------------------------------------------------------------------------\n",
        "conectado = es_conectado( laberinto, ( 1, 1 ), ( tamaño - 2, tamaño - 2 ) )\n",
        "while not conectado:\n",
        "    laberinto = generar_laberinto( tamaño, obstaculos )\n",
        "    conectado = es_conectado( laberinto, ( 1, 1 ), ( tamaño - 2, tamaño - 2 ) )\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Visualizar el laberinto antes de comenzar el entrenamiento\n",
        "#----------------------------------------------------------------------------\n",
        "plt.figure( figsize = ( 8, 8 ) )  # Tamaño de la figura\n",
        "plt.imshow( np.where( laberinto == '#', 0, 1 ), cmap = 'gray', origin = 'upper' )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Etiquetar la posición inicial y final\n",
        "#----------------------------------------------------------------------------\n",
        "posicion_inicial = np.where( laberinto == \"S\" )\n",
        "posicion_final = np.where( laberinto == \"M\" )\n",
        "plt.text( posicion_inicial[ 1 ], posicion_inicial[ 0 ], \"S\", ha = 'center', va = 'center', fontsize = 12, fontweight = 'bold', color = 'red' )\n",
        "plt.text( posicion_final[ 1 ], posicion_final[ 0 ], \"M\", ha = 'center', va = 'center', fontsize = 12, fontweight = 'bold', color = 'green' )\n",
        "\n",
        "plt.title( 'Laberinto inicial' )\n",
        "plt.xticks( np.arange( tamaño ), [] )\n",
        "plt.yticks( np.arange( tamaño ), [] )\n",
        "plt.grid( True, color = 'black' )\n",
        "plt.show()\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Inicialización de la tabla Q\n",
        "#----------------------------------------------------------------------------\n",
        "Q = np.zeros( ( laberinto.shape[ 0 ], laberinto.shape[ 1 ], len( acciones ) ) )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Parámetros del algoritmo Q-learning\n",
        "#----------------------------------------------------------------------------\n",
        "alpha = 0.1     # Tasa de aprendizaje\n",
        "gamma = 0.8     # Factor de descuento\n",
        "epsilon = 0.2   # Probabilidad de exploración\n",
        "caminos = 1000  # Número de iteraciones de entrenamiento\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Entrenamiento del agente\n",
        "#----------------------------------------------------------------------------\n",
        "for _ in range( caminos ):\n",
        "    posicion = ( 1, 1 )  # Estado inicial\n",
        "    while laberinto[ posicion[ 0 ], posicion[ 1 ] ] != \"M\":  # Mientras no lleguemos a la salida\n",
        "        accion = elegir_accion( Q, epsilon, posicion )\n",
        "        proxima_posicion = ( posicion[ 0 ] + acciones[ accion ][ 0 ], posicion[ 1 ] + acciones[ accion ][ 1 ] )\n",
        "        if laberinto[ proxima_posicion[ 0 ], proxima_posicion[ 1 ] ] == \"#\":  # Si la próxima acción es una pared\n",
        "            proxima_posicion = posicion\n",
        "        if laberinto[ proxima_posicion[ 0 ], proxima_posicion[ 1 ] ] == \"M\":  # Si llegamos a la salida\n",
        "            recompensa = 100  # Recompensa alta por alcanzar la salida\n",
        "        else:\n",
        "            recompensa = -1\n",
        "        Q = actualizar_Q( Q, posicion, accion, recompensa, proxima_posicion, alpha, gamma )\n",
        "        posicion = proxima_posicion\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Encontramos el camino más corto usando la política aprendida (greedy)\n",
        "#----------------------------------------------------------------------------\n",
        "camino_mas_corto = encontrar_camino_mas_corto( Q, laberinto )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Visualizamos el laberinto con el camino más corto\n",
        "#----------------------------------------------------------------------------\n",
        "plt.figure( figsize = ( 8, 8 ) )  # Tamaño de la figura\n",
        "plt.imshow( np.where( laberinto == '#', 0, 1 ), cmap = 'gray', origin = 'upper' )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Etiquetamos las posiciones inicial y final\n",
        "#----------------------------------------------------------------------------\n",
        "for i, pos in enumerate( camino_mas_corto ):\n",
        "    if i == 0:\n",
        "        plt.text( pos[ 1 ], pos[ 0 ], \"S\", ha = 'center', va = 'center', fontsize = 12, fontweight = 'bold', color = 'red' )\n",
        "    elif i == len( camino_mas_corto ) - 1:\n",
        "        plt.text( pos[ 1 ], pos[ 0 ], \"M\", ha = 'center', va = 'center', fontsize = 12, fontweight = 'bold', color = 'green' )\n",
        "    else:\n",
        "        plt.text( pos[ 1 ], pos[ 0 ], str( i ), ha = 'center', va = 'center', fontsize = 8, fontweight = 'bold' )\n",
        "\n",
        "plt.title( 'Laberinto con el camino más corto encontrado mediante Q-learning' )\n",
        "plt.xticks( np.arange( tamaño ), [] )\n",
        "plt.yticks( np.arange( tamaño ), [] )\n",
        "plt.grid( True, color = 'black' )\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}